{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSE5243 - Introduction to Data Mining Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "STOP_WORDS = set(sw.words('english')) \n",
    "\n",
    "import string\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Vocabulary Construction\n",
    "\n",
    "Scan the dataset to find all the unique tokens (using NLTK's `word_tokenize` for tokenization). Sort the tokens by frequency and only keep the top 40K most frequent tokens in the vocabulary (there are over 200K unique tokens in total using the current tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Construction\n",
    "def construct_vocab(files,count_cv=0):\n",
    "    print(f'|   |   |   Constructing vocabulary')\n",
    "    start_time = time.time()\n",
    "\n",
    "    vocab_full = {}\n",
    "    n_doc = 0\n",
    "    for file in files:\n",
    "        n_doc += 1\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split into words\n",
    "                tokens = word_tokenize(line)\n",
    "                # convert to lower case\n",
    "                tokens = [w.lower() for w in tokens]\n",
    "                # remove punctuation from each word\n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                stripped = [w.translate(table) for w in tokens]\n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                words = [word for word in stripped if word.isalpha()]\n",
    "                # filter out stop words\n",
    "                words = [w for w in words if not w in STOP_WORDS]\n",
    "                # stemming of words\n",
    "                porter = PorterStemmer()\n",
    "                stemmed = [porter.stem(word) for word in words]\n",
    "                for token in stemmed:\n",
    "                    vocab_full[token] = vocab_full.get(token, 0) + 1\n",
    "    print(f'|   |   |   |   {n_doc} documents scaned')\n",
    "    print(f'|   |   |   |   Full vocabulary has {len(vocab_full)} words')\n",
    "    vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ideal_vocab_size = min(len(vocab_sorted),MAX_VOCAB_SIZE)\n",
    "    vocab_truncated = vocab_sorted[:ideal_vocab_size]\n",
    "    # Save the vocabulary to file for visual inspection and possible analysis\n",
    "    with open(f'vocab_{count_cv+1}.txt', 'w') as f:\n",
    "        for vocab, freq in vocab_truncated:\n",
    "            f.write(f'{vocab}\\t{freq}\\n')\n",
    "    # The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "    vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "    # Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "    vocab['UNK'] = ideal_vocab_size\n",
    "\n",
    "    print(f'|   |   |   |   Truncated vocabulary has {len(vocab)} words')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'|   |   |   Constructing vocabulary took {elapsed_time/60:.2f} minutes')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "\n",
    "Scan the dataset one more time to extract the feature vector and class label of each document.\n",
    "Note that it's possible to scan the entire dataset only once to both construct the vocabulary and extract the feature vectors. Because we also truncate the vocabulary, we choose to do two scans to make the code mode clear at the cost of runtime efficiency. If efficiency is an issue, you may optimize it by only doing one scan over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(files, vocab, label2id = 0):\n",
    "    print(f'|   |   |   Extracting feature')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "    features = np.zeros((len(files), len(vocab)), dtype=int)\n",
    "    print(f'|   |   |   |   Feature matrix takes {sys.getsizeof(features)/1000000:.4f} Mb')\n",
    "\n",
    "    # The class label of each document\n",
    "    labels = np.zeros(len(files), dtype=int)\n",
    "    # The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "    doc_id = 0\n",
    "    if label2id == 0:\n",
    "        folders = list(set(os.path.dirname(file) for file in files))\n",
    "        folder_name = [os.path.basename(folder) for folder in folders]\n",
    "        label2id = dict([(label, id) for id, label in enumerate(folder_name)])\n",
    "    for file in files:\n",
    "        folder = os.path.dirname(file)\n",
    "        label = os.path.basename(folder)\n",
    "        labels[doc_id] = label2id[label]\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split into words\n",
    "                tokens = word_tokenize(line)\n",
    "                # convert to lower case\n",
    "                tokens = [w.lower() for w in tokens]\n",
    "                # remove punctuation from each word\n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                stripped = [w.translate(table) for w in tokens]\n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                words = [word for word in stripped if word.isalpha()]\n",
    "                # filter out stop words\n",
    "                words = [w for w in words if not w in STOP_WORDS]\n",
    "                # stemming of words\n",
    "                porter = PorterStemmer()\n",
    "                stemmed = [porter.stem(word) for word in words]\n",
    "                for token in stemmed:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    unk_id = len(vocab) - 1\n",
    "                    token_id = vocab.get(token, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    # id2label = dict([(id, label) for label, id in label2id.items()])\n",
    "    dataset = np.column_stack((features, labels))\n",
    "    print(f'|   |   |   |   Dataset has dimension {dataset.shape}')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'|   |   |   Extracting feature took {elapsed_time/60:.2f} minutes')\n",
    "    return dataset, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds): \n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "# Get list of files\n",
    "def load_files(root_path):\n",
    "    print(f'|   Reading files from {root_path}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "    folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "    i = 0\n",
    "    list_files = list()\n",
    "    for folder in folders:\n",
    "        print(f'|   |   {i+1}) Reading data from {folder}')\n",
    "        files = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "        i += 1\n",
    "        for file in files:\n",
    "            list_files.append(file)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'|   Reading files took {elapsed_time:.2f} seconds')\n",
    "    print()\n",
    "    return list_files\n",
    "\n",
    "\n",
    "# Stratified cross-validation split\n",
    "def stratified_datasplit(root_path):\n",
    "    print(f'|   Starting stratified_datasplit')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "    folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "    folds = [[] for i in range(N_FOLDS)]\n",
    "    i = 0\n",
    "    for folder in folders:\n",
    "        print(f'|   |   {i+1}) Splitting data from {folder}')\n",
    "        files = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "        folds = np.column_stack((cross_validation_split(files, N_FOLDS),folds))\n",
    "        i += 1\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'|   stratified_datasplit took {elapsed_time:.2f} seconds')\n",
    "    print()\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        tmp = [row[-1] for row in group]\n",
    "        for class_val in classes:\n",
    "            p = tmp.count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    start_time = time.time()\n",
    "    # print(f'|   |   |   |   Finding a split')\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 0, 0, 1, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        iteration_start_time = time.time()\n",
    "        # print(f'|   |   |   |   |   {index+1}th attribute')\n",
    "        left = list()\n",
    "        tmp = [[row[index],row[-1]] for row in list(dataset)]\n",
    "        right = sorted(tmp, key = operator.itemgetter(0), reverse = True)\n",
    "        left.append(right.pop())\n",
    "        while (len(right) > MIN_SIZE) and ((len(left) < MIN_SIZE) or (right[-1][0] == left[-1][0])):\n",
    "            left.append(right.pop())\n",
    "        updated = False\n",
    "        length = len(right)\n",
    "        for i in range(length):\n",
    "            if (len(right) <= MIN_SIZE) or (right[0][0] == 0):\n",
    "                break\n",
    "            groups = left, right\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score = index, right[-1][0]-0.5, gini\n",
    "                updated = True\n",
    "                b_i = i\n",
    "            left.append(right.pop())\n",
    "            while (len(right) > MIN_SIZE) and (right[-1][0] == left[-1][0]) :\n",
    "                left.append(right.pop())\n",
    "        iteration_time = time.time() - iteration_start_time\n",
    "        ## if updated:\n",
    "            ## print(f\"|   |   |   |   |   Best Gini = {b_score:.6f} at {index+1}th attribute {str(b_i)}th row\")\n",
    "        # print(f\"|   |   |   |   |   |   Spent {iteration_time:.0f} seconds, Best Gini = {b_score:.6f} at {index+1}th attribute{(' '+str(b_i)+'th row') if updated else ', did not change'}\")\n",
    "    b_groups = test_split(b_index,b_value,dataset)\n",
    "    n_rows = [len(group) for group in b_groups]\n",
    "    acc = to_terminal(dataset)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    # print(f'|   |   |   |   Finding a split took {elapsed_time:.2f} seconds')\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups, 'left_rows': n_rows[0], 'right_rows': n_rows[1], 'accuracy': acc}\n",
    "\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    tmp = [max(set(outcomes), key=outcomes.count)]\n",
    "    predicted = tmp * len(group)\n",
    "    accuracy = accuracy_metric(outcomes, predicted)\n",
    "    return [tmp[0], accuracy, len(group)]\n",
    "\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    # print(f\"{(depth-1)*'  :  '}[X{node['index']+1} < {node['value']}]\") \n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        # print(f\"{depth*'  :  '}single root ... [{node['left']}]\")\n",
    "        return True\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = to_terminal(left)\n",
    "        # print(f\"{depth*'  :  '}max_depth ... [{node['left']}]\")\n",
    "        node['right'] = to_terminal(right)\n",
    "        # print(f\"{depth*'  :  '}max_depth ... [{node['right']}]\")\n",
    "        if node['left'][0] == node['right'][0]:\n",
    "            node['left'] = node['right'] = to_terminal(left + right)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # process left child\n",
    "    min_left = False\n",
    "    single_left = False\n",
    "    if len(left) <= 2*min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "        min_left = True\n",
    "        # print(f\"{depth*'  :  '}min_size on left... [{node['left']}]\")\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        # if min(node['left']['left_rows'],node['left']['right_rows']) <= min_size:\n",
    "        #     node['left'] = to_terminal(left)\n",
    "        #     min_left = True\n",
    "        #     # print(f\"{(depth+1)*'  :  '}single right ... [{node['right']}]\")\n",
    "        # else:\n",
    "        single_left = split(node['left'], max_depth, min_size, depth+1)\n",
    "        if single_left:\n",
    "            node['left'] = to_terminal(left)\n",
    "            # print(f\"{(depth+1)*'  :  '}single left ... [{node['left']}]\")\n",
    "    left_isleaf = min_left or single_left\n",
    "    \n",
    "    # process right child\n",
    "    min_right = False\n",
    "    single_right = False\n",
    "    if len(right) <= 2*min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "        min_right = True\n",
    "        # print(f\"{depth*'  :  '}min_size on right ... [{node['right']}]\")\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        # if min(node['right']['left_rows'],node['right']['right_rows']) <= min_size:\n",
    "        #     node['right'] = to_terminal(right)\n",
    "        #     min_right = True\n",
    "        #     # print(f\"{(depth+1)*'  :  '}single right ... [{node['right']}]\")\n",
    "        # else:\n",
    "        single_right = split(node['right'], max_depth, min_size, depth+1)\n",
    "        if single_right:\n",
    "            node['right'] = to_terminal(right)\n",
    "            # print(f\"{(depth+1)*'  :  '}single right ... [{node['right']}]\")\n",
    "    right_isleaf = min_right or single_right\n",
    "    \n",
    "    if left_isleaf and right_isleaf and node['left'][0] == node['right'][0]:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    single = False\n",
    "    if root['value'] == 0:\n",
    "        root = to_terminal(train)\n",
    "    else:\n",
    "        single = split(root, max_depth, min_size, 1)\n",
    "        if single:\n",
    "            root = to_terminal(train)\n",
    "    return root\n",
    "\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=1):\n",
    "    if isinstance(node, dict):\n",
    "        print(f\"{(depth-1)*'  │  '}  ├──[X{node['index']+1} < {node['value']}] . . . . . . {node['accuracy'][1]:.2f}% of {node['accuracy'][2]} rows\")\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree_right(node['right'], depth+1)\n",
    "    else:\n",
    "        print(f\"{(depth-1)*'  │  '}  ├──[{node[0]}] . . . . . . {node[1]:.2f}% of {node[2]} rows\")\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree_right(node, depth):\n",
    "    if isinstance(node, dict):\n",
    "        print(f\"{(depth-1)*'  │  '}  └──[X{node['index']+1} < {node['value']}] . . . . . . {node['accuracy'][1]:.2f}% of {node['accuracy'][2]} rows\")\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree_right(node['right'], depth+1)\n",
    "    else:\n",
    "        print(f\"{(depth-1)*'  │  '}  └──[{node[0]}] . . . . . . {node[1]:.2f}% of {node[2]} rows\")\n",
    "\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if not isinstance(node, dict):\n",
    "        return node[0]\n",
    "    else: \n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left'][0]\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right'][0]\n",
    " \n",
    "\n",
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    start_time = time.time()\n",
    "    print(f'|   |   |   Building Decision Tree')\n",
    "    tree = build_tree(train, max_depth, min_size)\n",
    "    build_time = time.time() - start_time\n",
    "    print(f'|   |   |   Building Decision Tree took {build_time/60:.2f} minutes')\n",
    "    print()\n",
    "    # print_tree(tree)\n",
    "    # print()\n",
    "    start_time = time.time()\n",
    "    print(f'|   |   |   Making predictions')\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    predict_time = time.time() - start_time\n",
    "    print(f'|   |   |   Making predictions took {predict_time:.2f} seconds')\n",
    "    return predictions, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    " \n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    " \n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    start_time = time.time()\n",
    "    print(f'|   |   |   Running KNN')\n",
    "    predictions = list()\n",
    "    i = 0\n",
    "    for row in test:\n",
    "        print(f'|   |   |   |   Predicting class for {i}th row')\n",
    "        predic_start_time = time.time()\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predic_time = time.time() - predic_start_time\n",
    "        print(f'|   |   |   |   |   Predicting class took {predic_time:.2f} seconds')\n",
    "        predictions.append(output)\n",
    "        i += 1\n",
    "    run_time = time.time() - start_time\n",
    "    print(f'|   |   |   Running KNN took {run_time/60:.2f} minutes')\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell for calibrating KNN and Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEIGHBORS = 20\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm_knn(root_path, algorithm, *args):\n",
    "    print(f'Evaluating {algorithm.__name__}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    folds = stratified_datasplit(root_path)\n",
    "    scores = list()\n",
    "    # count_cv = 0\n",
    "    for i in range(len(folds)):\n",
    "\n",
    "        # A switch to run the algorithm just once for efficiency\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        print(f'|   {i+1}th iteration')\n",
    "        train_set = list()\n",
    "        for j in range(len(folds)):\n",
    "            if not i == j:\n",
    "                train_set = np.append(train_set, copy.deepcopy(folds[j]))\n",
    "\n",
    "        print(f'|   |   Building feature matrix on training data')\n",
    "        train_vocab = construct_vocab(train_set)\n",
    "        # count_cv += 1\n",
    "        train_data, label2id = extract_feature(train_set, train_vocab)\n",
    "        minmax = dataset_minmax(train_data)\n",
    "        normalize_dataset(train_data,minmax)\n",
    "        print()\n",
    "\n",
    "        test_set = list(folds[i])\n",
    "        print(f'|   |   Building feature matrix on testing data')\n",
    "        test_data_with_label, _ = extract_feature(test_set, train_vocab, label2id)\n",
    "        normalize_dataset(test_data_with_label,minmax)\n",
    "        test_data_no_label = test_data_with_label[:,:-1]\n",
    "        print()\n",
    "\n",
    "        print(f'|   |   Running {algorithm.__name__}')\n",
    "        predicted = algorithm(train_data, test_data_no_label, *args)\n",
    "        print()\n",
    "        print(f'|   |   Caculating the accuracy')\n",
    "        actual = [row[-1] for row in test_data_with_label]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        print(f'|   |   Accuracy is {accuracy:.2f}%')\n",
    "        scores.append(accuracy)\n",
    "        print()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Evaluating {algorithm.__name__} took {elapsed_time/60:.2f} minutes')\n",
    "    print()\n",
    "    return scores\n",
    "\n",
    "# seed(1)\n",
    "# evaluate algorithm\n",
    "\n",
    "# scores= evaluate_algorithm_knn(train_path, k_nearest_neighbors, NUM_NEIGHBORS)\n",
    "\n",
    "# print(\"Scores:\", *(f\"{s:.3f}%\" for s in scores))\n",
    "# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# print()\n",
    "# run_time = time.time() - run_start_time\n",
    "# print(f'The program ran for {run_time/60:.2f} minutes')\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell for calibrating the decision tree and Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_start_time = time.time()\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "N_FOLDS = 5 #10\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "train_path = \"./20news-train\"\n",
    "test_path = \"./20news-test\"\n",
    "\n",
    "MAX_DEPTH = 400000\n",
    "MIN_SIZE = 15\n",
    "\n",
    "# seed(1)\n",
    "# evaluate algorithm\n",
    "\n",
    "# scores= evaluate_algorithm_tree(train_path, decision_tree, MAX_DEPTH, MIN_SIZE)\n",
    "\n",
    "# print(\"Scores:\", *(f\"{s:.3f}%\" for s in scores))\n",
    "# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# print()\n",
    "# run_time = time.time() - run_start_time\n",
    "# print(f'The program ran for {run_time/60:.2f} minutes')\n",
    "# print('Done!')\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm_tree(root_path, algorithm, *args):\n",
    "    print(f'Evaluating {algorithm.__name__}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    folds = stratified_datasplit(root_path)\n",
    "    scores = list()\n",
    "    # count_cv = 0\n",
    "    for i in range(len(folds)):\n",
    "\n",
    "        # A switch to run the algorithm just once for efficiency\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        print(f'|   {i+1}th iteration')\n",
    "        train_set = list()\n",
    "        for j in range(len(folds)):\n",
    "            if not i == j:\n",
    "                train_set = np.append(train_set, copy.deepcopy(folds[j]))\n",
    "\n",
    "        print(f'|   |   Building feature matrix on training data')\n",
    "        train_vocab = construct_vocab(train_set)\n",
    "        # count_cv += 1\n",
    "        train_data, label2id = extract_feature(train_set, train_vocab)\n",
    "        minmax = dataset_minmax(train_data)\n",
    "        normalize_dataset(train_data,minmax)\n",
    "        print()\n",
    "\n",
    "        test_set = list(folds[i])\n",
    "        print(f'|   |   Building feature matrix on testing data')\n",
    "        test_data_with_label, _ = extract_feature(test_set, train_vocab, label2id)\n",
    "        normalize_dataset(test_data_with_label,minmax)\n",
    "        test_data_no_label = test_data_with_label[:,:-1]\n",
    "        print()\n",
    "\n",
    "        print(f'|   |   Running {algorithm.__name__}')\n",
    "        predicted, _ = algorithm(train_data, test_data_no_label, *args)\n",
    "        print()\n",
    "        print(f'|   |   Caculating the accuracy')\n",
    "        actual = [row[-1] for row in test_data_with_label]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        print(f'|   |   Accuracy is {accuracy:.2f}%')\n",
    "        scores.append(accuracy)\n",
    "        print()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Evaluating {algorithm.__name__} took {elapsed_time/60:.2f} minutes')\n",
    "    print()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For running hidden set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_set = load_files(train_path)\n",
    "test_set = load_files(test_path)\n",
    "\n",
    "print(f'|   |   Building feature matrix on training data from: {train_path}')\n",
    "train_vocab = construct_vocab(train_set)\n",
    "train_data, label2id = extract_feature(train_set, train_vocab)\n",
    "\n",
    "print(f'|   |   Building feature matrix on testing data from: {test_path}')\n",
    "test_data_with_label, _ = extract_feature(test_set, train_vocab, label2id)\n",
    "test_data_no_label = test_data_with_label[:,:-1]\n",
    "\n",
    "print(f'|   |   Running decision_tree')\n",
    "predicted, tree = decision_tree(train_data, test_data_no_label, MAX_DEPTH, MIN_SIZE)\n",
    "\n",
    "print(f'|   |   Caculating the accuracy')\n",
    "actual = [row[-1] for row in test_data_with_label]\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(f'|   |   Accuracy is {accuracy:.2f}%')\n",
    "\n",
    "print_tree(tree)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Evaluating decision_tree took {elapsed_time/60:.2f} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
