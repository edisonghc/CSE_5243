{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSE5243 - Introduction to Data Mining Assignment 3 and 4\n",
    "\n",
    "In case you encountered any issue in Assignment 2 and the feature matrix is not usable for Assignemnt 3 and 4, you could build on this starter code in order to get meaningful results for Assignment 3 and 4. \n",
    "\n",
    "Note that the preprocessing done in this starter code is minimal, just basic tokenization and vocabulary truncation. In order to get good classification and clustering results, it's a good idea to employ more preprocessing techniques, such as [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), feature selection (dimensionality reduction), and data transformation (e.g., normalization). \n",
    "\n",
    "This requires NLTK and Python 3 (tested with Python 3.8). If you are using Python 2.7 (deprecated), you will likely need to adapt it accordingly, e.g., the f-string formatting in `print`s and some list comprehension syntax.\n",
    "\n",
    "PLEASE KEEP THIS CODE CONFIDENFIAL TO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /Users/edisongu/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer as ps\n",
    "from nltk.corpus import stopwords as sw\n",
    "STOP_WORDS = set(sw.words('english')) \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Stratified cross-validation split\n",
    "def stratified_datasplit(root_path):\n",
    "    # Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "    folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "    folds = [[] for i in range(N_FOLDS)]\n",
    "    for folder in folders:\n",
    "        files = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "        folds = np.column_stack(cross_validation_split(files, N_FOLDS),folds)\n",
    "    return folds\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(root_path, algorithm, *args):\n",
    "\tfolds = stratified_datasplit(root_path)\n",
    "\tscores = list()\n",
    "\tbuild_time = list()\n",
    "\tpredict_time = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttrain_vocab = construct_vocab(train_set)\n",
    "\t\ttrain_data, label2id = extract_feature(train_set, train_vocab)\n",
    "\n",
    "\t\ttest_set = list(fold)\n",
    "\t\ttest_data_with_label, _ = extract_feature(test_set, train_vocab, label2id)\n",
    "\t\ttest_data_no_label = test_data[:,:-1]\n",
    "\n",
    "\t\tpredicted, build_t, predict_t = algorithm(train_data, test_data_no_label, *args)\n",
    "\t\tactual = [row[-1] for row in test_data_with_label]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\t\tbuild_time.append(build_t)\n",
    "\t\tpredict_time.append(predict_t)\n",
    "\treturn scores, build_time, predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Vocabulary Construction\n",
    "\n",
    "Scan the dataset to find all the unique tokens (using NLTK's `word_tokenize` for tokenization). Sort the tokens by frequency and only keep the top 40K most frequent tokens in the vocabulary (there are over 200K unique tokens in total using the current tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "11314 documents in total with a total vocab size of 208302\nVocabulary construction took 66.63518500328064 seconds\n"
    }
   ],
   "source": [
    "# Vocabulary Construction\n",
    "def construct_feature(files):\n",
    "    start_time = time.time()\n",
    "    vocab_full = {}\n",
    "    n_doc = 0\n",
    "    for file in files:\n",
    "        n_doc += 1\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                filtered_tokens = [w for w in tokens if not w in STOP_WORDS]\n",
    "                for token in filtered_tokens:\n",
    "                    root_word = ps.stem(token)\n",
    "                    vocab_full[root_word] = vocab_full.get(root_word, 0) + 1\n",
    "    print(f'{n_doc} documents in total with a total vocab size of {len(vocab_full)}')\n",
    "    vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ideal_vocab_size = min(len(vocab_sorted),MAX_VOCAB_SIZE)\n",
    "    vocab_truncated = vocab_sorted[:ideal_vocab_size]\n",
    "    # Save the vocabulary to file for visual inspection and possible analysis\n",
    "    with open('vocab.txt', 'w') as f:\n",
    "        for vocab, freq in vocab_truncated:\n",
    "            f.write(f'{vocab}\\t{freq}\\n')\n",
    "    # The final vocabulary is a dict mapping each token to its id. frequency information is not     needed anymore.\n",
    "    vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "    # Since we have truncated the vocabulary, we will encounter many tokens that are not in the     vocabulary. We will map all of them to the same 'UNK' token (a common practice in text          processing), so we append it to the end of the vocabulary.\n",
    "    vocab['UNK'] = ideal_vocab_size\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Vocabulary construction took {elapsed_time} seconds')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Feature Extraction\n",
    "\n",
    "Scan the dataset one more time to extract the feature vector and class label of each document.\n",
    "Note that it's possible to scan the entire dataset only once to both construct the vocabulary and extract the feature vectors. Because we also truncate the vocabulary, we choose to do two scans to make the code mode clear at the cost of runtime efficiency. If efficiency is an issue, you may optimize it by only doing one scan over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The feature matrix takes 9141824 Bytes.\nFeature extraction took 75.99522376060486 seconds\n"
    }
   ],
   "source": [
    "def extract_feature(files, vocab, label2id = 0):\n",
    "    # Since we have truncated the vocabulary, it's now reasonable to hold the entire feature        matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you         could make the vocabulary even smaller or use sparse matrix.\n",
    "    start_time = time.time()\n",
    "    features = np.zeros((len(files), len(vocab)), dtype=int)\n",
    "    print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "    # The class label of each document\n",
    "    labels = np.zeros(len(files), dtype=int)\n",
    "    # The mapping from the name of each class label (i.e., the subdictionary name corresponding     to a topic) to an integer ID\n",
    "    doc_id = 0\n",
    "    if label2id == 0:\n",
    "        folders = list(set(os.path.dirname(file) for file in files))\n",
    "        label2id = dict{[(label, id) for id, label in enumerate(folders)]}\n",
    "    for file in files:\n",
    "        label = os.path.dirname(file)\n",
    "        labels[doc_id] = label2id[label]\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                filtered_tokens = [w for w in tokens if not w in STOP_WORDS]\n",
    "                for token in filtered_tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get                         the ID of the UNK token\n",
    "                    root_word = ps.stem(token)\n",
    "                    unk_id = len(vocab) - 1\n",
    "                    token_id = vocab.get(root_word, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Feature extraction took {elapsed_time} seconds')\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    # id2label = dict([(id, label) for label, id in label2id.items()])\n",
    "    dataset = np.column_stack((features, labels))\n",
    "    return dataset, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classification - Decision Tree\n",
    "\n",
    "Reference: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfor index in range(len(dataset[0])-1):\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left)\n",
    "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right)\n",
    "\t\tsplit(node['right'], max_depth, min_siwze, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "\troot = get_split(train)\n",
    "\tsplit(root, max_depth, min_size, 1)\n",
    "\treturn root\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "\tif isinstance(node, dict):\n",
    "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "\t\tprint_tree(node['left'], depth+1)\n",
    "\t\tprint_tree(node['right'], depth+1)\n",
    "\telse:\n",
    "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    start_time = time.time()\n",
    "\ttree = build_tree(train, max_depth, min_size)\n",
    "    build_time = time.time() - start_time\n",
    "\n",
    "    print_tree(tree)\n",
    "\n",
    "    start_time = time.time()\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(tree, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "    predict_time = time.time() - start_time\n",
    "\treturn predictions, build_time, predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a08ab899b4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_DEPTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbuilding_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-698bf1bd6d90>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(train, max_depth, min_size)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Build a decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-698bf1bd6d90>\u001b[0m in \u001b[0;36mget_split\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                         \u001b[0mgini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgini_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mgini\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                 \u001b[0mb_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-698bf1bd6d90>\u001b[0m in \u001b[0;36mgini_index\u001b[0;34m(groups, classes)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;31m# score the group based on the score for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mclass_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# weight the group score by its relative size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 10 #40000\n",
    "N_FOLDS = 5 #10\n",
    "\n",
    "MAX_DEPTH = 5\n",
    "MIN_SIZE = 10\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-train\"\n",
    "\n",
    "# Test CART on Bank Note dataset\n",
    "seed(1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "\n",
    "scores, build_time, predict_time = evaluate_algorithm(root_path, decision_tree, MAX_DEPTH, MIN_SIZE)\n",
    "\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "print(build_time)\n",
    "print(predict_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an empty cell\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}