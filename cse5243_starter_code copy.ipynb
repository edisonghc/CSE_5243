{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starter Code for CSE5243 - Introduction to Data Mining Assignment 3 and 4\n",
    "\n",
    "In case you encountered any issue in Assignment 2 and the feature matrix is not usable for Assignemnt 3 and 4, you could build on this starter code in order to get meaningful results for Assignment 3 and 4. \n",
    "\n",
    "Note that the preprocessing done in this starter code is minimal, just basic tokenization and vocabulary truncation. In order to get good classification and clustering results, it's a good idea to employ more preprocessing techniques, such as [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), feature selection (dimensionality reduction), and data transformation (e.g., normalization). \n",
    "\n",
    "This requires NLTK and Python 3 (tested with Python 3.8). If you are using Python 2.7 (deprecated), you will likely need to adapt it accordingly, e.g., the f-string formatting in `print`s and some list comprehension syntax.\n",
    "\n",
    "PLEASE KEEP THIS CODE CONFIDENFIAL TO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Vocabulary Construction\n",
    "\n",
    "Scan the dataset to find all the unique tokens (using NLTK's `word_tokenize` for tokenization). Sort the tokens by frequency and only keep the top 40K most frequent tokens in the vocabulary (there are over 200K unique tokens in total using the current tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-train\"\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "\n",
    "start_time = time.time()\n",
    "vocab_full = {}\n",
    "n_doc = 0\n",
    "# Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "        n_doc += 1\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    vocab_full[token] = vocab_full.get(token, 0) + 1\n",
    "print(f'{n_doc} documents in total with a total vocab size of {len(vocab_full)}')\n",
    "vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocab_truncated = vocab_sorted[:MAX_VOCAB_SIZE]\n",
    "# Save the vocabulary to file for visual inspection and possible analysis\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for vocab, freq in vocab_truncated:\n",
    "        f.write(f'{vocab}\\t{freq}\\n')\n",
    "# The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "# Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "vocab['UNK'] = MAX_VOCAB_SIZE\n",
    "vocab_size = len(vocab)\n",
    "unk_id = MAX_VOCAB_SIZE\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Vocabulary construction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Feature Extraction\n",
    "\n",
    "Scan the dataset one more time to extract the feature vector and class label of each document.\n",
    "Note that it's possible to scan the entire dataset only once to both construct the vocabulary and extract the feature vectors. Because we also truncate the vocabulary, we choose to do two scans to make the code mode clear at the cost of runtime efficiency. If efficiency is an issue, you may optimize it by only doing one scan over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "start_time = time.time()\n",
    "features = np.zeros((n_doc, vocab_size), dtype=int)\n",
    "print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "# The class label of each document\n",
    "labels = np.zeros(n_doc, dtype=int)\n",
    "# The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "label2id = {}\n",
    "label_id = 0\n",
    "doc_id = 0\n",
    "for folder in folders:\n",
    "    label2id[folder] = label_id\n",
    "    for filename in os.listdir(folder):\n",
    "        labels[doc_id] = label_id\n",
    "        file = os.path.join(folder, filename)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    token_id = vocab.get(token, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    label_id += 1\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Feature extraction took {elapsed_time} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}